# Redis 고가용성 전략

Redis의 가용성(HA)을 위한 전략은 크게 **Replication**, **Sentinel**, **Cluster**, **RedLock** 네 가지로 구분되며, 각 전략은 **데이터 복제 방식, 장애 감지 및 자동 복구 수준, 확장성** 측면에서 차이가 있으며, 운영 환경과 요구사항에 따라 적절히 선택해야 한다.

## 1. Replication

- **구조**

  - **Master–Slave(마스터–레플리카)** 구조 기반
  - 하나의 Master 서버와 하나 이상의 Slave 서버로 구성

- **동작 원리**

  - Master 서버: 모든 쓰기(Write) 작업 처리
  - Slave 서버: Master 서버의 데이터를 비동기적으로 복제
  - Slave는 Master 데이터를 복제해 “읽기(Read) 작업” 분산 처리

- **주요 장점**

  - 읽기 확장성 향상
    - 다수의 Slave 서버가 읽기 요청을 분산 처리
  - 데이터 가용성 확보
    - Master 서버 장애 시에도 Slave 서버에서 데이터 조회 가능

- **주요 단점 / 발생 가능한 문제점**
  - **비동기 복제 → 데이터 손실 가능**
    - Master에서 쓰기가 끝나기 전에 장애 발생 시, 아직 전파되지 않은 데이터가 사라질 수 있음
  - **자동 장애 조치(failover) 미지원**
    - Master 장애 시 Slave 승격은 수동 또는 외부 도구가 필요

---

## 2. Sentinel

- **목적**

  - Replication 기반 환경에서 “모니터링 + 자동 장애 조치”를 제공하여 HA 보장

- **동작 원리**

  1. **모니터링**
     - Sentinel 인스턴스들이 Master/Slave 서버 상태를 주기적으로 체크
  2. **장애 감지 → 자동 장애 조치(failover)**
     - Master 서버 장애 감지 시, Sentinel 쿼럼(과반수 합의)을 통해  
       새로운 Master(Server)로 적합한 Slave를 자동 승격
     - 나머지 Slave들은 새 Master를 따르도록 재구성
  3. **클라이언트 리디렉션**
     - Sentinel은 클라이언트에게 새로운 Master 주소를 알려줘서  
       애플리케이션이 자동으로 연결할 수 있도록 지원
  4. **알림**
     - 장애 발생 시 시스템 관리자에게 알림 전송 가능

- **주요 장점**

  - 자동 장애 조치 → 관리자가 개입하지 않아도 Master 교체
  - Sentinel 자체가 “설정 제공자(Configuration Provider)” 역할을 함
    - 클라이언트가 Sentinel에 질의해 현재 Master 정보를 실시간 조회 가능

- **주요 단점 / 발생 가능한 문제점**
  - **Sentinel 인스턴스 운영 복잡도**
    - 안정적인 장애 감지를 위해 **최소 3대 이상** 구성 권장
    - 각각 다른 서버나 가용 영역에 분리 배포해야 오탐(false positive) 방지
  - **단일 Master 구조**
    - 쓰기 확장은 불가능 → 한 번에 하나의 Master만 존재
  - **네트워크 파티션/쿼럼 문제**
    - 잘못된 설정이나 네트워크 분할 시 잘못된 Master 승격(스플릿 브레인) 발생 가능

---

## 3. Cluster

- **목적**

  - “샤딩(데이터 분산 저장) + 복제 + 자동 장애 조치”를 결합한 내장 분산 아키텍처

- **동작 원리**

  1. **해시 슬롯(Hash Slot) 분할**
     - 전체 키 공간을 **16,384개 슬롯**으로 분할
     - 각 마스터 노드가 담당하는 슬롯 범위를 가짐
  2. **샤딩(데이터 분산)**
     - 클라이언트는 키를 해싱해 어느 노드(슬롯)에 저장할지 결정
     - `-MOVED` 또는 `-ASK` 리디렉션으로 올바른 노드에 요청 전송
  3. **복제 & 장애 조치**
     - 각 마스터 노드마다 하나 이상의 리플리카(Replica)가 존재
     - 마스터 장애 시 해당 슬롯을 담당하는 리플리카가 자동으로 승격 → 연속성 유지
  4. **Gossip 프로토콜**
     - 노드들 간 상태 정보를 빠르게 전파
     - 과반수 쿼럼 기준으로 장애 감지 및 복구 결정

- **주요 장점**

  - **수평 확장성**
    - 노드 추가로 용량·처리량 확장 가능
  - **부분적 장애 허용**
    - 일부 마스터 노드가 다운되더라도, 나머지 과반수 노드가 살아있으면 서비스 유지
  - **자동 리밸런싱**
    - 노드 추가/제거 시 슬롯 재분배가 자동으로 이루어져 데이터 균형 유지

- **주요 단점 / 발생 가능한 문제점**
  - **복잡한 운영**
    - 클러스터 토폴로지 관리, 슬롯 재분배, 클러스터 재구성 등 작업이 까다로움
    - 클라이언트도 클러스터 모드를 지원해야 함
  - **멀티-키 연산 제약**
    - 서로 다른 슬롯에 있는 키끼리는 원자적(Transactional) 연산 불가
  - **비동기 복제 → 쓰기 손실 가능**
    - 쓰기 커맨드가 마스터에서 리플리카로 전파되기 전에 장애 시 일부 데이터 손실
  - **과반수 노드 장애 시 전체 서비스 불가**
    - 클러스터가 살아 있으려면 과반수 마스터 노드가 반드시 온라인 상태여야 함

---

## 4. RedLock

- **목적**

  - “분산 환경에서 충돌 없이 안전하게 락(Lock) 획득”을 보장하기 위한 알고리즘 패턴

- **동작 원리**

  1. **N개의 독립 Redis 인스턴스 준비**
     - 보통 최소 3~5개 Redis 노드를 서로 복제 없이 개별 운영
  2. **락 획득 시도**
     - 클라이언트가 각 인스턴스에 순차적으로  
       `SET resource_name <token> NX PX <ttl>` 명령 실행
     - **TTL**(만료 시간)을 짧게 설정하여 락이 오래 걸리지 않도록 함
     - 일정 시간 내 **과반수(majority)** 인스턴스에서 성공 응답을 받으면 락 획득 완료
     - 과반수를 얻지 못하면 이미 설정된 락을 모두 해제(롤백) 후 재시도
  3. **락 해제**
     - 클라이언트가 보유한 `<token>`을 Lua 스크립트로 검사한 뒤,  
       락 키를 삭제하여 안전하게 해제
  4. **락 연장 (Optional)**
     - 락 만료 전에 필요한 경우, 과반수 인스턴스에 다시 `SET NX PX` 순차 요청하여 TTL 연장

- **주요 장점**

  - **과반수 노드 동의 → 상호 배제 보장**
    - 일부 Redis 인스턴스가 장애나 네트워크 분리되어도  
      과반수만 살아 있으면 락이 유지되므로 안전성 확보
  - **분산 환경에서의 신뢰성 있는 락**
    - 단일 Redis에 의존하지 않고, 다수 인스턴스 비교를 통해 잠금 충돌 방지

- **주요 단점 / 발생 가능한 문제점**
  - **네트워크 지연(network latency)** 민감
    - 락 획득 중 메시지 지연이 길어지면 **과반수 응답 획득 실패** → 락 불안정
  - **클럭 드리프트(clock drift)** 주의
    - 서로 다른 서버 시계 오차로 인해 TTL 계산이 틀려질 수 있음
  - **추가 운영 오버헤드**
    - N개의 독립 Redis를 별도 구성 및 모니터링 필요
    - 클라이언트 측 락 알고리즘 구현 복잡

---
